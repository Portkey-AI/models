{
  "default": {
    "messages": {
      "options": [
        "system",
        "user",
        "assistant"
      ]
    },
    "params": [
      {
        "defaultValue": 1024,
        "key": "max_tokens",
        "maxValue": 128000,
        "minValue": 1
      },
      {
        "defaultValue": 0.7,
        "key": "temperature",
        "maxValue": 2,
        "minValue": 0
      },
      {
        "defaultValue": 1,
        "key": "top_p",
        "maxValue": 1,
        "minValue": 0
      },
      {
        "defaultValue": null,
        "key": "stop",
        "skipValues": [
          null,
          []
        ],
        "type": "array-of-strings"
      },
      {
        "defaultValue": true,
        "key": "stream",
        "type": "boolean"
      }
    ],
    "pricing": {
      "calculate": {
        "request": {
          "operands": [
            {
              "value": "input_tokens"
            },
            {
              "value": "rates.request_token"
            }
          ],
          "operation": "multiply"
        },
        "response": {
          "operands": [
            {
              "value": "output_tokens"
            },
            {
              "value": "rates.response_token"
            }
          ],
          "operation": "multiply"
        }
      },
      "currency": "USD"
    },
    "type": {
      "primary": "chat",
      "supported": []
    }
  },
  "description": "",
  "models": {
    "gpt-oss-120b": {
      "max_output_tokens": 32768,
      "params": [
        {
          "defaultValue": 1024,
          "key": "max_tokens",
          "maxValue": 32768,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        }
      ],
      "pricing": {
        "input": 0.35,
        "output": 0.75
      },
      "removeParams": [
        "stream"
      ],
      "type": "chat"
    },
    "llama-3.3-70b": {
      "capabilities": [
        "tools"
      ],
      "max_output_tokens": 32768,
      "params": [
        {
          "defaultValue": 1024,
          "key": "max_tokens",
          "maxValue": 32768,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        }
      ],
      "pricing": {
        "input": 0.85,
        "output": 1.2
      },
      "type": "chat"
    },
    "llama-4-maverick-17b-128e-instruct": {
      "capabilities": [
        "tools"
      ],
      "max_output_tokens": 32768,
      "params": [
        {
          "defaultValue": 1024,
          "key": "max_tokens",
          "maxValue": 32768,
          "minValue": 1
        },
        {
          "defaultValue": 0.6,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 0.9,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        }
      ],
      "pricing": {
        "input": 0.2,
        "output": 0.6
      },
      "type": "chat"
    },
    "llama-4-scout-17b-16e-instruct": {
      "capabilities": [
        "tools"
      ],
      "max_output_tokens": 32768,
      "params": [
        {
          "defaultValue": 1024,
          "key": "max_tokens",
          "maxValue": 32768,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        }
      ],
      "pricing": {
        "input": 0.65,
        "output": 0.85
      },
      "type": "chat"
    },
    "llama3.1-70b": {
      "max_output_tokens": 128000,
      "pricing": {
        "input": 0.6,
        "output": 0.6
      },
      "type": "chat"
    },
    "llama3.1-8b": {
      "capabilities": [
        "tools"
      ],
      "max_output_tokens": 32768,
      "params": [
        {
          "defaultValue": 1024,
          "key": "max_tokens",
          "maxValue": 32768,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        }
      ],
      "pricing": {
        "input": 0.1,
        "output": 0.1
      },
      "type": "chat"
    },
    "qwen-3-235b-a22b-instruct-2507": {
      "capabilities": [
        "tools"
      ],
      "max_output_tokens": 32768,
      "params": [
        {
          "defaultValue": 1024,
          "key": "max_tokens",
          "maxValue": 32768,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        }
      ],
      "pricing": {
        "input": 0.6,
        "output": 1.2
      },
      "type": "chat"
    },
    "qwen-3-235b-a22b-thinking-2507": {
      "capabilities": [
        "tools"
      ],
      "max_output_tokens": 32768,
      "params": [
        {
          "defaultValue": 1024,
          "key": "max_tokens",
          "maxValue": 32768,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        }
      ],
      "pricing": {
        "input": 0.6,
        "output": 1.2
      },
      "type": "chat"
    },
    "qwen-3-32b": {
      "capabilities": [
        "tools"
      ],
      "max_output_tokens": 32768,
      "params": [
        {
          "defaultValue": 1024,
          "key": "max_tokens",
          "maxValue": 32768,
          "minValue": 1
        },
        {
          "defaultValue": 0.6,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 0.95,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        }
      ],
      "pricing": {
        "input": 0.4,
        "output": 0.8
      },
      "type": "chat"
    },
    "qwen-3-coder-480b": {
      "capabilities": [
        "tools"
      ],
      "max_output_tokens": 32768,
      "params": [
        {
          "defaultValue": 1024,
          "key": "max_tokens",
          "maxValue": 32768,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 0.8,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        }
      ],
      "pricing": {
        "input": 2,
        "output": 2
      },
      "type": "chat"
    },
    "zai-glm-4.6": {
      "max_output_tokens": 128000,
      "pricing": {
        "input": 2.25,
        "output": 2.75
      },
      "type": "chat"
    }
  },
  "provider": "cerebras",
  "provider_name": "cerebras"
}
