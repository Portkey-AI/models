{
  "default": {
    "messages": {
      "options": [
        "system",
        "user",
        "assistant"
      ]
    },
    "params": [
      {
        "defaultValue": 256,
        "key": "max_tokens",
        "maxValue": 8192,
        "minValue": 1
      },
      {
        "defaultValue": 0.7,
        "key": "temperature",
        "maxValue": 2,
        "minValue": 0
      },
      {
        "defaultValue": 1,
        "key": "top_p",
        "maxValue": 1,
        "minValue": 0
      },
      {
        "defaultValue": 1,
        "key": "n",
        "maxValue": 10,
        "minValue": 1
      },
      {
        "defaultValue": 0,
        "key": "frequency_penalty",
        "maxValue": 2,
        "minValue": -2
      },
      {
        "defaultValue": 0,
        "key": "presence_penalty",
        "maxValue": 2,
        "minValue": -2
      },
      {
        "defaultValue": null,
        "key": "stop",
        "skipValues": [
          null,
          []
        ],
        "type": "array-of-strings"
      },
      {
        "defaultValue": true,
        "key": "stream",
        "type": "boolean"
      },
      {
        "defaultValue": null,
        "key": "tool_choice",
        "options": [
          {
            "name": "None",
            "value": "none"
          },
          {
            "name": "Auto",
            "value": "auto"
          },
          {
            "name": "Required",
            "value": "required"
          },
          {
            "name": "Custom",
            "schema": {
              "type": "json"
            },
            "value": "custom"
          }
        ],
        "rule": {
          "default": {
            "condition": "tools",
            "else": null,
            "then": "auto"
          }
        },
        "skipValues": [
          null,
          []
        ],
        "type": "non-view-manage-data"
      }
    ],
    "pricing": {
      "calculate": {
        "request": {
          "operands": [
            {
              "value": "input_tokens"
            },
            {
              "value": "rates.request_token"
            }
          ],
          "operation": "multiply"
        },
        "response": {
          "operands": [
            {
              "value": "output_tokens"
            },
            {
              "value": "rates.response_token"
            }
          ],
          "operation": "multiply"
        }
      },
      "currency": "USD"
    },
    "type": {
      "primary": "chat",
      "supported": [
        "image"
      ]
    }
  },
  "description": "",
  "models": {
    "01-ai/Yi-34B-Chat": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "Austism/chronos-hermes-13b-v2": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "BAAI/bge-base-en-v1.5": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "BAAI/bge-en-icl": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "BAAI/bge-large-en-v1.5": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "BAAI/bge-m3": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "BAAI/bge-m3-multi": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Bria/Bria-3.2": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Bria/Bria-3.2-vector": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Bria/blur_background": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Bria/enhance": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Bria/erase": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Bria/erase_foreground": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Bria/expand": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Bria/fibo": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Bria/gen_fill": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Bria/remove_background": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Bria/replace_background": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "ByteDance/SeeDance-T2V": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "ByteDance/Seedream-4": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "image"
    },
    "DeepInfra/pygmalion-13b-4bit-128g": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "EleutherAI/pythia-2.8b": {
      "max_output_tokens": 4096,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 4096,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "Gryphe/MythoMax-L2-13b": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.08,
        "output": 0.08
      },
      "type": "chat"
    },
    "MiniMaxAI/MiniMax-M2": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.27,
        "output": 1.15
      },
      "type": "chat"
    },
    "NousResearch/Hermes-3-Llama-3.1-405B": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 1,
        "output": 1
      },
      "type": "chat"
    },
    "NousResearch/Hermes-3-Llama-3.1-70B": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.3,
        "output": 0.3
      },
      "type": "chat"
    },
    "PaddlePaddle/PaddleOCR-VL-0.9B": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.14,
        "output": 0.8
      },
      "type": "chat"
    },
    "Phind/Phind-CodeLlama-34B-v2": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "PrunaAI/p-image": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "PrunaAI/p-image-Edit": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Qwen/QwQ-32B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Qwen/Qwen-Image-Edit": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Qwen/Qwen2.5-72B-Instruct": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.12,
        "output": 0.39
      },
      "type": "chat"
    },
    "Qwen/Qwen2.5-7B-Instruct": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Qwen/Qwen2.5-Coder-32B-Instruct": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Qwen/Qwen2.5-VL-32B-Instruct": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.2,
        "output": 0.6
      },
      "type": "chat"
    },
    "Qwen/Qwen3-14B": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.08,
        "output": 0.24
      },
      "type": "chat"
    },
    "Qwen/Qwen3-235B-A22B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Qwen/Qwen3-235B-A22B-Instruct-2507": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.071,
        "output": 0.463
      },
      "type": "chat"
    },
    "Qwen/Qwen3-235B-A22B-Thinking-2507": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.3,
        "output": 2.9
      },
      "type": "chat"
    },
    "Qwen/Qwen3-30B-A3B": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.08,
        "output": 0.29
      },
      "type": "chat"
    },
    "Qwen/Qwen3-32B": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.1,
        "output": 0.28
      },
      "type": "chat"
    },
    "Qwen/Qwen3-Coder-480B-A35B-Instruct": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.4,
        "output": 1.6
      },
      "type": "chat"
    },
    "Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.29,
        "output": 1.2
      },
      "type": "chat"
    },
    "Qwen/Qwen3-Embedding-0.6B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "embedding"
    },
    "Qwen/Qwen3-Embedding-0.6B-batch": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "embedding"
    },
    "Qwen/Qwen3-Embedding-4B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "embedding"
    },
    "Qwen/Qwen3-Embedding-4B-batch": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "embedding"
    },
    "Qwen/Qwen3-Embedding-8B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "embedding"
    },
    "Qwen/Qwen3-Embedding-8B-batch": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "embedding"
    },
    "Qwen/Qwen3-Next-80B-A3B-Instruct": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.14,
        "output": 1.1
      },
      "type": "chat"
    },
    "Qwen/Qwen3-Reranker-0.6B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Qwen/Qwen3-Reranker-4B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Qwen/Qwen3-Reranker-8B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Qwen/Qwen3-VL-235B-A22B-Instruct": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.2,
        "output": 1.2
      },
      "type": "chat"
    },
    "Qwen/Qwen3-VL-30B-A3B-Instruct": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.15,
        "output": 0.6
      },
      "type": "chat"
    },
    "ResembleAI/chatterbox": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Sao10K/L3-8B-Lunaris-v1-Turbo": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.04,
        "output": 0.05
      },
      "type": "chat"
    },
    "Sao10K/L3.1-70B-Euryale-v2.2": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.85,
        "output": 0.85
      },
      "type": "chat"
    },
    "Sao10K/L3.3-70B-Euryale-v2.3": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.85,
        "output": 0.85
      },
      "type": "chat"
    },
    "Wan-AI/Wan2.1-T2V-1.3B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Wan-AI/Wan2.1-T2V-14B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Zyphra/Zonos-v0.1-hybrid": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "Zyphra/Zonos-v0.1-transformer": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "allenai/olmOCR-2-7B-1025": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.09,
        "output": 0.19
      },
      "type": "chat"
    },
    "allenai/olmOCR-7B-0725-FP8": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "anthropic/claude-3-7-sonnet-latest": {
      "max_output_tokens": 8192,
      "pricing": {
        "cache_read": 0.33,
        "input": 3.3,
        "output": 16.5
      },
      "type": "chat"
    },
    "anthropic/claude-4-opus": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 16.5,
        "output": 82.5
      },
      "type": "chat"
    },
    "anthropic/claude-4-sonnet": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 3.3,
        "output": 16.5
      },
      "type": "chat"
    },
    "black-forest-labs/FLUX-1-Redux-dev": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "black-forest-labs/FLUX-1-dev": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "black-forest-labs/FLUX-1-schnell": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "black-forest-labs/FLUX-1.1-pro": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "black-forest-labs/FLUX-2-dev": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "image"
    },
    "black-forest-labs/FLUX-2-pro": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "image"
    },
    "black-forest-labs/FLUX-pro": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "black-forest-labs/FLUX.1-Kontext-dev": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "canopylabs/orpheus-3b-0.1-ft": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "codellama/CodeLlama-34b-Instruct-hf": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "codellama/CodeLlama-70b-Instruct-hf": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "cognitivecomputations/dolphin-2.6-mixtral-8x7b": {
      "max_output_tokens": 32768,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 32768,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "deepinfra/airoboros-70b": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-OCR": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.03,
        "output": 0.1
      },
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-R1": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-R1-0528": {
      "max_output_tokens": 8192,
      "pricing": {
        "cache_read": 0.4,
        "input": 0.5,
        "output": 2.15
      },
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-R1-0528-Turbo": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 1,
        "output": 3
      },
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-R1-Distill-Llama-70B": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.6,
        "output": 1.2
      },
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-R1-Turbo": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-V3": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.38,
        "output": 0.89
      },
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-V3-0324": {
      "max_output_tokens": 8192,
      "pricing": {
        "cache_read": 0.106,
        "input": 0.2,
        "output": 0.88
      },
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-V3-0324-Turbo": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-V3.1": {
      "max_output_tokens": 8192,
      "pricing": {
        "cache_read": 0.168,
        "input": 0.21,
        "output": 0.79
      },
      "type": "chat"
    },
    "deepseek-ai/DeepSeek-V3.1-Terminus": {
      "max_output_tokens": 8192,
      "pricing": {
        "cache_read": 0.168,
        "input": 0.21,
        "output": 0.79
      },
      "type": "chat"
    },
    "deepseek-ai/Janus-Pro-1B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "deepseek-ai/Janus-Pro-7B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "google/embeddinggemma-300m": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "embedding"
    },
    "google/gemini-1.5-flash": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.075,
        "output": 0.3
      },
      "type": "chat"
    },
    "google/gemini-1.5-flash-8b": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.0375,
        "output": 0.15
      },
      "type": "chat"
    },
    "google/gemini-2.0-flash-001": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.1,
        "output": 0.4
      },
      "type": "chat"
    },
    "google/gemini-2.5-flash": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.3,
        "output": 2.5
      },
      "type": "chat"
    },
    "google/gemini-2.5-pro": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 1.25,
        "output": 10
      },
      "type": "chat"
    },
    "google/gemma-3-12b-it": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.04,
        "output": 0.13
      },
      "type": "chat"
    },
    "google/gemma-3-27b-it": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.09,
        "output": 0.16
      },
      "type": "chat"
    },
    "google/gemma-3-4b-it": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.04,
        "output": 0.08
      },
      "type": "chat"
    },
    "google/gemma-7b-it": {
      "max_output_tokens": 4096,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 4096,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "hexgrad/Kokoro-82M": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "intfloat/e5-base-v2": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "intfloat/e5-large-v2": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "intfloat/multilingual-e5-large": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "intfloat/multilingual-e5-large-instruct": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "jondurbin/airoboros-l2-70b-gpt4-1.4.1": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "lizpreciatior/lzlv_70b_fp16_hf": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.35,
        "output": 0.4
      },
      "type": "chat"
    },
    "llava-hf/llava-1.5-7b-hf": {
      "max_output_tokens": 4096,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 4096,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "meta-llama/Llama-2-13b-chat-hf": {
      "max_output_tokens": 4096,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 4096,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "meta-llama/Llama-2-70b-chat-hf": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "meta-llama/Llama-2-7b-chat-hf": {
      "max_output_tokens": 4096,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 4096,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "meta-llama/Llama-3.2-11B-Vision-Instruct": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.049,
        "output": 0.049
      },
      "type": "chat"
    },
    "meta-llama/Llama-3.2-3B-Instruct": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.02,
        "output": 0.02
      },
      "type": "chat"
    },
    "meta-llama/Llama-3.2-90B-Vision-Instruct": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "meta-llama/Llama-3.3-70B-Instruct": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "meta-llama/Llama-3.3-70B-Instruct-Turbo": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.13,
        "output": 0.38
      },
      "type": "chat"
    },
    "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.15,
        "output": 0.6
      },
      "type": "chat"
    },
    "meta-llama/Llama-4-Maverick-17B-128E-Instruct-Turbo": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "meta-llama/Llama-4-Scout-17B-16E-Instruct": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.08,
        "output": 0.3
      },
      "type": "chat"
    },
    "meta-llama/Llama-Guard-3-8B": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "meta-llama/Llama-Guard-4-12B": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.18,
        "output": 0.18
      },
      "type": "chat"
    },
    "meta-llama/Meta-Llama-3-70B-Instruct": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.23,
        "output": 0.4
      },
      "type": "chat"
    },
    "meta-llama/Meta-Llama-3-8B-Instruct": {
      "max_output_tokens": 4096,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 4096,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.03,
        "output": 0.06
      },
      "type": "chat"
    },
    "meta-llama/Meta-Llama-3.1-405B-Instruct": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.09,
        "output": 0.09
      },
      "type": "chat"
    },
    "meta-llama/Meta-Llama-3.1-70B-Instruct": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.4,
        "output": 0.4
      },
      "type": "chat"
    },
    "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.4,
        "output": 0.4
      },
      "type": "chat"
    },
    "meta-llama/Meta-Llama-3.1-8B-Instruct": {
      "max_output_tokens": 4096,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 4096,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.03,
        "output": 0.05
      },
      "type": "chat"
    },
    "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.02,
        "output": 0.03
      },
      "type": "chat"
    },
    "meta-llama/Meta-Llama-3.3-70B-Instruct": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.23,
        "output": 0.04
      },
      "type": "chat"
    },
    "meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.12,
        "output": 0.03
      },
      "type": "chat"
    },
    "microsoft/WizardLM-2-7B": {
      "max_output_tokens": 4096,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 4096,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.055,
        "output": 0.055
      },
      "type": "chat"
    },
    "microsoft/WizardLM-2-8x22B": {
      "max_output_tokens": 8192,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 8192,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.48,
        "output": 0.48
      },
      "type": "chat"
    },
    "microsoft/phi-4": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.07,
        "output": 0.14
      },
      "type": "chat"
    },
    "mistralai/Devstral-Small-2507": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "mistralai/Mistral-7B-Instruct-v0.1": {
      "capabilities": [
        "tools"
      ],
      "max_output_tokens": 4096,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 4096,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        },
        {
          "defaultValue": null,
          "key": "response_format",
          "options": [
            {
              "name": "Text",
              "value": null
            },
            {
              "name": "JSON Object",
              "schema": {
                "properties": {
                  "type": {
                    "type": "string",
                    "value": "json_object"
                  }
                },
                "type": "object"
              },
              "value": "json_object"
            }
          ],
          "skipValues": [
            null
          ],
          "type": "string"
        }
      ],
      "pricing": null,
      "type": "chat"
    },
    "mistralai/Mistral-7B-Instruct-v0.3": {
      "max_output_tokens": 4096,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 4096,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.03,
        "output": 0.055
      },
      "type": "chat"
    },
    "mistralai/Mistral-Nemo-Instruct-2407": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.02,
        "output": 0.04
      },
      "type": "chat"
    },
    "mistralai/Mistral-Small-24B-Instruct-2501": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.05,
        "output": 0.08
      },
      "type": "chat"
    },
    "mistralai/Mistral-Small-3.1-24B-Instruct-2503": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "mistralai/Mistral-Small-3.2-24B-Instruct-2506": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.075,
        "output": 0.2
      },
      "type": "chat"
    },
    "mistralai/Mixtral-8x7B-Instruct-v0.1": {
      "capabilities": [
        "tools"
      ],
      "max_output_tokens": 32768,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 32768,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.54,
        "output": 0.54
      },
      "type": "chat"
    },
    "mistralai/Voxtral-Mini-3B-2507": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "mistralai/Voxtral-Small-24B-2507": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "moonshotai/Kimi-K2-Instruct": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "moonshotai/Kimi-K2-Instruct-0905": {
      "max_output_tokens": 8192,
      "pricing": {
        "cache_read": 0.4,
        "input": 0.5,
        "output": 2
      },
      "type": "chat"
    },
    "moonshotai/Kimi-K2-Thinking": {
      "max_output_tokens": 8192,
      "pricing": {
        "cache_read": 0.141,
        "input": 0.47,
        "output": 2
      },
      "type": "chat"
    },
    "nvidia/Llama-3.1-Nemotron-70B-Instruct": {
      "max_output_tokens": 8192,
      "pricing": {
        "input": 1.2,
        "output": 1.2
      },
      "type": "chat"
    },
    "nvidia/Llama-3.3-Nemotron-Super-49B-v1.5": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.1,
        "output": 0.4
      },
      "type": "chat"
    },
    "nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.2,
        "output": 0.6
      },
      "type": "chat"
    },
    "nvidia/NVIDIA-Nemotron-Nano-9B-v2": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.04,
        "output": 0.16
      },
      "type": "chat"
    },
    "openai/clip-vit-base-patch32": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "openai/clip-vit-large-patch14-336": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "openai/gpt-oss-120b": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.039,
        "output": 0.19
      },
      "type": "chat"
    },
    "openai/gpt-oss-120b-Turbo": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.15,
        "output": 0.6
      },
      "type": "chat"
    },
    "openai/gpt-oss-20b": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "input": 0.03,
        "output": 0.14
      },
      "type": "chat"
    },
    "openai/whisper-large-v3": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "openai/whisper-large-v3-turbo": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "openchat/openchat_3.5": {
      "max_output_tokens": 4096,
      "params": [
        {
          "defaultValue": 256,
          "key": "max_tokens",
          "maxValue": 4096,
          "minValue": 1
        },
        {
          "defaultValue": 0.7,
          "key": "temperature",
          "maxValue": 2,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "top_p",
          "maxValue": 1,
          "minValue": 0
        },
        {
          "defaultValue": 1,
          "key": "n",
          "maxValue": 10,
          "minValue": 1
        },
        {
          "defaultValue": 0,
          "key": "frequency_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": 0,
          "key": "presence_penalty",
          "maxValue": 2,
          "minValue": -2
        },
        {
          "defaultValue": null,
          "key": "stop",
          "skipValues": [
            null,
            []
          ],
          "type": "array-of-strings"
        },
        {
          "defaultValue": true,
          "key": "stream",
          "type": "boolean"
        },
        {
          "defaultValue": null,
          "key": "tool_choice",
          "options": [
            {
              "name": "None",
              "value": "none"
            },
            {
              "name": "Auto",
              "value": "auto"
            },
            {
              "name": "Required",
              "value": "required"
            },
            {
              "name": "Custom",
              "schema": {
                "type": "json"
              },
              "value": "custom"
            }
          ],
          "rule": {
            "default": {
              "condition": "tools",
              "else": null,
              "then": "auto"
            }
          },
          "skipValues": [
            null,
            []
          ],
          "type": "non-view-manage-data"
        }
      ],
      "pricing": {
        "input": 0.055,
        "output": 0.055
      },
      "type": "chat"
    },
    "sentence-transformers/all-MiniLM-L12-v2": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "sentence-transformers/all-MiniLM-L6-v2": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "sentence-transformers/all-mpnet-base-v2": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "sentence-transformers/clip-ViT-B-32": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "sentence-transformers/clip-ViT-B-32-multilingual-v1": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "sentence-transformers/multi-qa-mpnet-base-dot-v1": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "sentence-transformers/paraphrase-MiniLM-L6-v2": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "sesame/csm-1b": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "shibing624/text2vec-base-chinese": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "stabilityai/sd3.5": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "stabilityai/sd3.5-medium": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "stabilityai/sdxl-turbo": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "thenlper/gte-base": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "thenlper/gte-large": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "zai-org/GLM-4.5": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "zai-org/GLM-4.5-Air": {
      "max_output_tokens": 8192,
      "pricing": null,
      "type": "chat"
    },
    "zai-org/GLM-4.6": {
      "disablePlayground": true,
      "max_output_tokens": 8192,
      "pricing": {
        "cache_read": 0.0799999993,
        "input": 0.43,
        "output": 1.75
      },
      "type": "chat"
    }
  },
  "provider": "deepinfra",
  "provider_name": "deepinfra"
}
